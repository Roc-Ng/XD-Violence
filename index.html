
<!-- saved from url=(0035)https://sdolivia.github.io/FineGym/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/analytics.js.下载"></script><script src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:16px;
		margin-left: auto;
		margin-right: auto;
		width: 800px;
	}
	
	h1 {
		font-weight:300;
	}
		
	h2 {
		font-weight:300;
		font-size: 22px;
		text-align: left;
	}

	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	pre {
    text-align: left;
    white-space: pre;
	background-color: ghostwhite;
	border: 1px solid #CCCCCC;
	padding: 10px 20px;
	margin: 10px;
    tab-size:         4; /* Chrome 21+, Safari 6.1+, Opera 15+ */
    -moz-tab-size:    4; /* Firefox 4+ */
    -o-tab-size:      4; /* Opera 11.5 & 12.1 only */
  	}

</style>


  
		<title>XD-Violence</title>
		<meta property="og:image" content="">
		<meta property="og:title" content="Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision">
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:42px">Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision</span>
	  		  <table align="center" width="800px">
	  			  <tbody><tr>
	  	              <td align="center" width="100px">
	  					<center>
	  						<span style="font-size:24px"><a href="https://roc-ng.github.io/">Peng Wu</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="100px">
	  					<center>
	  						<span style="font-size:24px"><a href="https://faculty.xidian.edu.cn/LJ22/zh_CN/lwcg/339624/list/index.htm">Jing Liu</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="100px">
	  					<center>
	  						<span style="font-size:24px"><a href="https://roc-ng.github.io/XD-Violence/">Yujia Shi</a></span>
		  		  		</center>
		  		  	  </td>
					<td align="center" width="100px">
						<center>
							<span style="font-size:24px"><a href="https://roc-ng.github.io/XD-Violence/">Yujia Sun</a></span>
						</center>
					</td>
					<td align="center" width="100px">
						<center>
							<span style="font-size:24px"><a href="https://roc-ng.github.io/XD-Violence/">Fangtao Shao</a></span>
						</center>
					</td>
					<td align="center" width="100px">
						<center>
							<span style="font-size:24px"><a href="https://roc-ng.github.io/XD-Violence/">Zhaoyang Wu</a></span>
						</center>
					</td>
					<td align="center" width="100px">
						<center>
							<span style="font-size:24px"><a href="https://roc-ng.github.io/XD-Violence/">Zhiwei Yang</a></span>
						</center>
					</td>
				</tr></tbody></table>
          		<!-- <span style="font-size:30px">ECCV 2016.</span> -->

			  <table align="center" width="600px">
				  <tbody><tr>
					  <td align="center" width="100px">
						<center>
							<span style="font-size:20px"><a href="https://en.xidian.edu.cn/">Xidian University</a></span>
						</center>
					  </td>
			  </tr></tbody></table>
			EUROPEAN CONFERENCE ON COMPUTER VISION (<a href="https://eccv2020.eu/" target="_blank">ECCV</a>) 2020, <font color="#e86e14">Poster Presentation</font>
          </center>

   		  <br><br>
		  <hr>

  		  <br>
  		  <table align="center" width="720px">
  			  <tbody><tr>
  	              <td width="400px">
  					<center>
  	                	<a href="./images/samples.png"><img class="rounded" src="./images/samples.png" width="800px"></a><br>
					</center>
  	              </td>
                </tr>
  	              <tr><td width="400px">
  					<center>
  	                	<span style="font-size:14px"><i>Sample videos from the XD-Violence dataset.</i>
					</span></center>
  	              </td>

  		  </tr></tbody></table>
      	  <br><br>
		  <hr>


  		  <center><h1>Abstract</h1></center><table align="center" width="720px">
				
		  </table>
		  <center>
			<span>
		 	On public benchmarks, current action recognition techniques have achieved great success.
		  	However, when used in real-world applications, e.g. sport analysis, which requires the capability of parsing an activity into phases and differentiating between subtly different actions, their performances remain far from being satisfactory.
		  	To take action recognition to a new level, we develop <i>FineGym</i>, a new dataset built on top of gymnasium videos.
		  	Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity.
		  	In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy.
		  	For example, a <i>"balance beam"</i> event will be annotated as a sequence of elementary sub-actions derived from five sets:
		  	<i>"leap-jumphop"</i>, <i>"beam-turns"</i>, <i>"flight-salto"</i>, <i>"flight-handspring"</i>, and <i>"dismount"</i>, 
		  	where the sub-action in each set will be further annotated with finely defined class labels.
		  	This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes.
		  	We systematically investigate representative methods on this dataset and obtain a number of interesting findings. We hope this dataset could advance research towards action understanding.
		  	</span>
		  </center>
  		  <br><br>
		  <hr>

		  <center><h1>Demo video</h1></center><table align="center" width="720px">
			
			<tbody><tr>
				</tr></tbody></table><table align="center" width="720px">
					<tbody><tr>
						<td align="center" width="720px">
							<iframe width="600" height="320" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/notLDzBJ2mg.html" frameborder="0" allowfullscreen=""></iframe>
						</td>
					  </tr>
					<tr>
						<td align="center" width="720px">
						  <span style="font-size:14px"><i>
							An illustrative video of FineGym's hiecharcial annotations given a complete competition.
							Action and subaction boundaries are highlighted while irrelevant fragments are fast-forwarded.
							We also present the tree-based process at the end of the demo video.</i>
						</span>
						   </td>
					  </tr>
					 </tbody></table>
			  
		  
		   <br><br>
		  <hr>

		  <center><h1>Dataset hierarchy</h1></center><table align="center" width="720px">
			
			<tbody><tr>
				<td width="400px">
				  <center>
					  <a><img class="rounded" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/hierarchy.png" width="800px"></a><br>
				</center>
				</td>
			</tr>
				<tr><td width="400px">
				  <center>
					  <span style="font-size:14px"><i>
						FineGym organizes both the semantic and temporal annotations hierarchically.
						The upper part shows three levels of categorical labels, namely events (e.g. balance beam), sets (e.g. dismounts) and elements (e.g. salto forward tucked).
						The lower part depicts the two-level temporal annotations, i.e. the temporal boundaries of actions (in the top bar) and sub-action instances (in the bottom bar).
						</i>
				</span></center>
				</td>

		  </tr></tbody></table>
	      <br><br>
		  <hr>

		  <center><h1>Sub-action examples</h1></center><center>
				<span>
					We present several examples of fine-grained sub-action instances.
					Each group belongs to three element categories within a same event (BB, FX, UB, and VT).
					It can be seen such fine-grained instances contain subtle and challenging differences.
					(Hover on the GIF for a 0.25x slowdown)</span>
				<br>
				</center><table align="center" width="720px">
			
			<tbody><tr>
				
				<td width="360px">
					<center>
						<span style="font-size:22px"><a>Balance Beam (BB)</a></span><br>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_bb_01_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_bb_01_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_bb_01_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_bb_02_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_bb_02_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_bb_02_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_bb_03_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_bb_03_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_bb_03_normal.gif" width="100px"></a>
					</center>
				</td>
				<td width="360px">
					<center>
						<span style="font-size:22px"><a>Floor Exercise (FX)</a></span><br>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_fx_01_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_fx_01_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_fx_01_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_fx_02_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_fx_02_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_fx_02_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_fx_03_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_fx_03_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_fx_03_normal.gif" width="100px"></a>
					</center>
				</td>
			</tr>
			<tr>
				<td width="360px">
					<center>
						<span style="font-size:22px"><a>Uneven Bar (UB)</a></span><br>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_ub_01_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_ub_01_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_ub_01_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_ub_02_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_ub_02_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_ub_02_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_ub_03_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_ub_03_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_ub_03_normal.gif" width="100px"></a>
					</center>
				</td>
				<td width="360px">
					<center>
						<span style="font-size:22px"><a>Vault (VT)</a></span><br>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_vt_01_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_vt_01_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_vt_01_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_vt_02_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_vt_02_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_vt_02_normal.gif" width="100px"></a>
						<a><img onmouseover="this.src=&#39;./resources/examples/example_vt_03_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/example_vt_03_normal.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/example_vt_03_normal.gif" width="100px"></a>
					</center>
				</td>
			</tr>
		  </tbody></table>

		  <br><br>
		  <hr>

		  <center><h1>Empirical Studies and Analysis</h1></center><table align="center" width="720px">
			
			</table><center><h2> (1) Element-level action recognition raises great challenges for existing methods. </h2></center><table>
			
			<tbody><tr>
				  <td width="400px">
				  <center>
					  <a><img class="rounded" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/element_recognition.png" width="800px"></a><br>
				</center>
				</td>
			</tr>
			<tr>
				<td align="center" width="720px">
				  <span style="font-size:14px"><i>
					Element-level action recognition results of representative methods.</i>
				</span>
				</td>
			</tr>
			</tbody></table>

			<br>

			<center><h2> (2) Sparse sampling is insufficient for fine-grained action recognition. </h2></center><table>
			
			<tbody><tr>
				  <td width="400px">
				  <center>
					  <a><img class="rounded" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/tsn_vs_nframe.png" width="320px"></a><br>
				</center>
				</td>
			</tr>
			<tr>
				<td align="center" width="720px">
				  <span style="font-size:14px"><i>
					Performances of TSN when varying the number of sampled frames during training.</i>
				</span>
				</td>
			</tr>
			</tbody></table>

			<br>

			<center><h2> (3) How important is temporal information? </h2></center>
				(a) Motion features (e.g. optical flows) could capture frame-wise temporal dynamics, leading to better performance of TSN.<br>
				(b) Temporal dynamics play an important role in FineGym, and TRN could capture it.<br>
				(c) Performance of TSM drops sharply when the number of testing frames is very different from that in training,
				while TSN maintains its performance as only temporal average pooling is applied in it.<br><table>
				
				<tbody><tr>
					  <td width="400px">
					  <center>
						  <a><img class="rounded" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/temporal_importance.png" width="800px"></a><br>
					</center>
					</td>
				</tr>
				<tr>
					<td align="center" width="720px">
					  <span style="font-size:14px"><i>
						(a) Per-class performances of TSN with motion and appearance features in 6 element categories. <br>
						(b) Performances of TRN on the set UB-circles using ordered or shuffled testing frames.<br>
						(c) Mean-class accuracies of TSM and TSN on Gym99 when trained with 3 frames and tested with more frames.</i>
					</span>
					</td>
				</tr>
				</tbody></table>

			<br>

			<center><h2> (4) Does pre-training on large-scale video datasets help? </h2></center>
				On FineGym, pre-training on Kinetics is not always helpful.
				One potential reason is the large gaps in terms of temporal patterns between coarse- and fine-grained actions. 
				<table>
				<tbody><tr>
					<td width="400px">
					<center>
						<a><img class="rounded" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/pretrain.png" width="240px"></a>
					</center>
					</td>
				</tr>
				<tr>
					<td align="center" width="720px">
					<span style="font-size:14px"><i>
					Per-class performances of I3D pre-trained on Kinetics and ImageNet in various element categories.</i>
					</span>
					</td>
				</tr>
				</tbody></table>

			<br>
			
			<center><h2> (4) Why pose information does not help? </h2></center>
				Skeleton-based ST-GCN struggles due to the challenges in skeleton estimation on gymnastics instances.
				<table>
				<tbody><tr>
					<td width="400px">
					<center>
						<a><img onmouseover="this.src=&#39;./resources/examples/pose_slow.gif&#39;;" onmouseout="this.src=&#39;./resources/examples/pose.gif&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/pose.gif" width="400px"></a>
					</center>
					</td>
				</tr>
				<tr>
					<td align="center" width="720px">
					<span style="font-size:14px"><i>
					The results of person detection and pose estimation using <a href="https://github.com/MVIG-SJTU/AlphaPose">AlphaPose</a> for a Vault routine.
					It can be seen that detections and pose estimations of the gymnast are missed in multiple frames, especially in frames with intense motion.
					These frames are important for fine-grained recognition. (Hover on the GIF for a 0.25x slowdown) </i>
					</span>
					</td>
				</tr>
				</tbody></table>
					
		
	      <br><br>
		  <hr>


		  <center><h1>Download</h1></center><table id="download" align="center" width="720px">
			
			<tbody><tr>
				<td width="300px">
					<center>
						<span style="font-size:24px">Categories</span><br>
						<br>
						<img class="rounded" onmouseover="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" onmouseout="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/dataset_icon.jpg" height="150px"><br><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/finegym_glabel_to_Qtree.json">question annotation (json)</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/set_categories.txt">set-level category list (txt)</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym99_categories.txt">Gym99 category list (txt)</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym288_categories.txt">Gym288 category list (txt)</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym530_categories.txt">Gym530 category list (txt)</a></span><br>
					<span style="font-size:16px"></span>
					</center>
				</td>
				<td width="300px">
					<center>
						<span style="font-size:24px">v1.0</span><br>
						<br>
						<img class="rounded" onmouseover="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" onmouseout="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/dataset_icon.jpg" height="150px"><br><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/finegym_annotation_info_v1.0.json">temporal annotation (json)</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym99_train_element_v1.0.txt">Gym99 train split</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym99_val_element.txt">Gym99 val split</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym288_train_element_v1.0.txt">Gym288 train split</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym288_val_element.txt">Gym288 val split</a></span><br>
					<span style="font-size:16px"></span>
					</center>
				</td>
				<td width="300px">
					<center>
						<span style="font-size:24px">v1.1</span><br><br>
						<img class="rounded" onmouseover="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" onmouseout="this.src=&#39;./resources/images/dataset_icon.jpg&#39;;" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/dataset_icon.jpg" height="150px"><br><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/finegym_annotation_info_v1.1.json">temporal annotation (json)</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym99_train_element_v1.1.txt">Gym99 train split</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym99_val_element.txt">Gym99 val split (same as v1.0)</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym288_train_element_v1.1.txt">Gym288 train split</a></span><br>
						<span style="font-size:16px"><a href="https://sdolivia.github.io/FineGym/resources/dataset/gym288_val_element.txt">Gym288 val split (same as v1.0)</a></span><br>
					<span style="font-size:16px"></span>
					</center>
				</td>
				</tr></tbody></table><center><h2> Updates </h2></center>
				[16/04/2020] We fix a small issue on the naming of the subaction identifier "A_{ZZZZ}_{WWWW}" to avoid ambiguity.
				(Thanks <a href="https://kennymckormick.github.io/">Haodong Duan</a> for pointing this out.)<br>
				[16/04/2020] We include new subsections to track updates and address FAQs.<br><center><h2> FAQs </h2></center>
				Q0: License issue:<br>
				A0: The annotations of FineGym are copyright by us and published under the Creative Commons Attribution-NonCommercial 4.0 International License.<br>
				Q1: Some links are invalid on YouTube. How can I obtain the missing videos?<br>
				Q1': I am located in mainland China and I cannot access YouTube. How can I get the dataset?<br>
				A1: Please submit a Google form at <a href="https://forms.gle/z8iTwRhdMsfRCNkm7">this link</a>.
				We may reach you shortly.<br>
				Q2: Is the event-/element-level instance in your dataset cut in integral seconds?<br>
				A2: No. All levels of instances (actions and sub-actions) are annotated in <i>exact</i> timestamp (milliseconds) 
				in a pursuit of frame-level preciseness.
				The number in the identifier is derived from integral seconds due to conciseness.
				Please refer to the instructions below for details.
				<center><h2> How to read the temporal annotation files (JSON)? </h2></center> 
				Below, we show an example entry from the above JSON annotation file:
				<pre style="font-family: Courier; font-size:14px">"0LtLS9wROrk": {
	"E_002407_002435": {
		"event": 4,
		"segments": {
			"A_0003_0005": {
				"stages": 1,
				"timestamps": [
					[
						3.45,
						5.64
					]
				]
			},
			"A_0006_0008": { ... },
			"A_0023_0028": { ... },
			...
		},
		"timestamps": [
			[
				2407.32,
				2435.28
			]
		]
	},
	"E_002681_002688": {
		"event": 1,
		"segments": {
			"A_0000_0006": {
				"stages": 3,
				"timestamps": [
					[
						0.04,
						3.2
					],
					[
						3.2,
						4.49
					],
					[
						4.49,
						6.57
					]
				]
			}
		},
		"timestamps": [
			[
				2681.88,
				2688.48
			]
		]
	},
	"E_002710_002737": { ... },
	...
}
				</pre><table>
				
				
				</table>
				The example shows the annotations related to this video.
				First of all, we assign the unique identifier <a style="font-family: Courier">"0LtLS9wROrk"</a> to that video,
				which corresponds to the 11-digit YouTube identifier. <br>
				It contains all action (event-level) instances, whose names follow the format of <a style="font-family: Courier">"E_{XXXXXX}_{YYYYYY}"</a>.
				Here, "E" indicates "Event", and "XXXXXX"/"YYYYYY" indicates the zero-padded starting and ending timestamp (in seconds and truncated to Int).
				<br>
				Each action instance includes (1) the exact timestamps in the original video ('timestamps', in seconds),
				(2) event label ('event'), and
				(3) a list of annotated subaction (element-level) instances ('segments').
				<br>
				The annotated subaction instances follow the format of <a style="font-family: Courier">"A_{ZZZZ}_{WWWW}"</a>.
				Here, "A" indicates "subAction", and "ZZZZ"/"WWWW" indicates the zero-padded starting and ending timestamp (in seconds and truncated to Int).
				<br>
				Ech subaction instance includes (1) the number of stages of this subaction instance ('stages', 3 for Vault and 1 for other events)
				(2) the exact timestamps of each stage <i>relative</i> to the starting time of event. ('timestamps', in seconds)
				As a result, each subaction instance has a unique identifier <a style="font-family: Courier">"{VIDEO_ID}_E_{XXXXXX}_{YYYYYY}_A_{ZZZZ}_{WWWW}"</a>.
				This identifier serves as the instance name in the train/val splits of Gym99 and Gym288.
			
		   

		   <center><h2> How to read the question annotation files (JSON)? </h2></center>
			Below, we show an example entry from the above JSON annotation file:
			<pre style="font-family: Courier; font-size:14px">"0": {
	"BTcode": "1111111",
	"questions": [
		"round-off onto the springboard?",
		"turning entry after round-off (turning in first flight phase)?",
		"Facing the coming direction when handstand on vault
		(0.5 turn in first flight phase)?",
		"Body keep stretched  during salto (stretched salto)?",
		"Salto with turn?",
		"Facing vault table after landing?",
		"Salto with 1.5 turn?"
	],
	"code": "6.00"
},
"1": {
	"BTcode": "1111110",
	"questions": [
		"round-off onto the springboard?",
		"turning entry after round-off (turning in first flight phase)?",
		"Facing the coming direction when handstand on vault
		(0.5 turn in first flight phase)?",
		"Body keep stretched  during salto (stretched salto)?",
		"Salto with turn?",
		"Facing vault table after landing?",
		"Salto with 1.5 turn?"
	],
	"code": "5.20"
},
...
			</pre><table>
			
			</table>
			The example shows the questions related to each class.
			The identifier corresponds to the label name provided in <a href="https://sdolivia.github.io/FineGym/resources/dataset/gym530_categories_wset.txt">Gym530 category list</a>.
			Each class includes (1) a list of questions that are asked ('quetions'),
			(2) a string of binary codes ('BTcode') where 1 refers to 'yes' and 0 refers to 'no',
			(3) and original code in the <a href="http://www.fig-gymnastics.com/publicdir/rules/files/en_WAG%20CoP%202017-2020.pdf">official codebook</a>.
		
	   		   
		 
		 <br><br>
		 <hr>

		 <center><h1>Paper</h1></center><table align="center" width="720px">
			
			   <tbody><tr>
				 <td align="center"><a href="https://sdolivia.github.io/FineGym/"><img class="layered-paper-big" style="height:160px" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/paper_pdf_thumb.png"></a></td>
				 <td><span style="font-size:14pt">Shao, Zhao, Dai, Lin.<br>
				 FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding<br>
				 In CVPR, 2020 (oral).<br>
				 (<a href="https://arxiv.org/abs/2004.06704">arXiv</a>)
				 <span style="font-size:4pt"><a href="https://sdolivia.github.io/FineGym/"><br></a>
				 </span>
				 </span></td>
				 <td align="center"><a href="https://sdolivia.github.io/FineGym/"><img class="layered-paper-big" style="height:160px" src="./FineGym_ A Hierarchical Video Dataset for Fine-grained Action Understanding_files/supp_pdf_thumb.png"></a></td>
				 <td><span style="font-size:14pt">
				 (<a href="https://sdolivia.github.io/FineGym/resources/supp.pdf">Additional details/<br>supplementary materials</a>)
				 <span style="font-size:4pt"><a href="https://sdolivia.github.io/FineGym/"><br></a>
				 </span>
				 </span></td>
			 </tr>
		   </tbody></table>
		 
		 <br><br>
		 <hr>

		  <center><h1>Cite</h1></center><div class="disclaimerbox">
			<!-- <center><h2>How to interpret the results</h2></center> -->

		   <span>
				<!-- <center><span style="font-size:28px"><b>Cite</b></span></center> -->
				<pre style="font-family:Courier; font-size:14px">@inproceedings{shao2020finegym,
title={FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding},
author={Shao, Dian and Zhao, Yue and Dai, Bo and Lin, Dahua},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2020}
}
				</pre>
		  </span></div><table align="center" width="720px">
			
		  
  		  </table>

			<br><br>
			<hr>
  
		  	
  		  <table align="center" width="720px">
  			  <tbody><tr>
  	              <td width="400px">
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
				We sincerely thank the outstanding annotation team for their excellent work.
				This work is partially supported by SenseTime Collaborative Grant on Large-scale Multi-modality Analysis
				and the General Research Funds (GRF) of Hong Kong (No. 14203518 and No. 14205719).
				The template of this webpage is borrowed from <a href="https://richzhang.github.io/colorization/">Richard Zhang</a><a>.
			</a></left><a>
		</a></td>
			 </tr>
		</tbody></table>

		<br><br>
		<hr>

		<table align="center" width="720px">
			<tbody><tr>
				<td width="400px">
				  <left>
			<center><h1>Contact</h1></center>
			For further questions and suggestions, please contact Dian Shao (<a href="mailto:sd017@ie.cuhk.edu.hk">sd017@ie.cuhk.edu.hk</a>).
			
			
		</left>
	</td>
		 </tr>
	</tbody></table>

		<br><br>

<script>
	
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-1', 'auto');
  ga('send', 'pageview');

</script>
              


 
</body></html>